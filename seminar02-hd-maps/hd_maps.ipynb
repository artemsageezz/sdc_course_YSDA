{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\newcommand{\\bolde}{\\boldsymbol{e}}\n",
    "\\newcommand{\\boldh}{\\boldsymbol{h}}\n",
    "\\newcommand{\\boldp}{\\boldsymbol{p}}\n",
    "\\newcommand{\\boldr}{\\boldsymbol{r}}\n",
    "\\newcommand{\\boldt}{\\boldsymbol{t}}\n",
    "\\newcommand{\\boldq}{\\boldsymbol{q}}\n",
    "\\newcommand{\\boldM}{\\boldsymbol{M}}\n",
    "\\newcommand{\\boldP}{\\boldsymbol{P}}\n",
    "\\newcommand{\\boldR}{\\boldsymbol{R}}\n",
    "\\newcommand{\\boldT}{\\boldsymbol{T}}\n",
    "\\newcommand{\\boldQ}{\\boldsymbol{Q}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High Definition (HD) Maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/hd_maps_01.jpg' width='100%'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/hd_maps_02.jpg' width='100%'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Настройка окружения\n",
    "\n",
    "Для выполнения кода из данного ноутбука лучше создать отдельное питоновское окружение. Проблема в том, что далее надо будет поставить довольно много библиотек, и, к сожалению, возможна ситуация возникнования конфликтов между ними.\n",
    "\n",
    "Существует два способа настроить питоновское окружение для запуска кода из данного ноутбука:\n",
    "1. С помощью `conda`\n",
    "2. С помощью `virtualenv`\n",
    "\n",
    "Полную информацию по управлению окружениями `conda` можно найти на [странице документации](\n",
    "https://conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html). Далее будет просто пошаговая инструкция конкретно для наших целей.\n",
    "\n",
    "Мы воспользуемся первым, как более простым. Предполагается, что `conda` у вас уже установлена. Для создания нового окружения с именем `sdc` и версией питона 3.9 выполняем команду:\n",
    "```shell\n",
    "conda create -n sdc python=3.9\n",
    "```\n",
    "\n",
    "Если все прошло успешно, то `sdc` появится в списке окружений:\n",
    "```shell\n",
    "conda env list\n",
    "```\n",
    "\n",
    "Теперь переключаемся в консоли на окружение `sdc`:\n",
    "```shell\n",
    "conda activate sdc\n",
    "```\n",
    "В результате в командной строке будем видеть нечто такое:\n",
    "```\n",
    "(sdc) ~$\n",
    "```\n",
    "\n",
    "Теперь, когда мы успешно переключились на окружение `sdc`, начинаем устанавливаеть нужные библиотеки.\n",
    "\n",
    "1. На всякий случай обновляем версии основных библиотек:\n",
    "    ```shell\n",
    "    pip install numpy --upgrade\n",
    "    pip install scipy --upgrade\n",
    "    pip install matplotlib --upgrade\n",
    "    ```\n",
    "2. Устанавливаем `jupyter lab` и `jupyter notebook` (`ipython-notebook`):\n",
    "```shell\n",
    "pip install jupyterlab\n",
    "pip install notebook\n",
    "```\n",
    "3. Теперь самое &laquo;интересное&raquo; &mdash; ставим библиотеки, которые нужны конкретно для данного ноутбука:\n",
    "    ```shell\n",
    "    pip install opencv-python\n",
    "    pip install plotly\n",
    "    pip install open3d\n",
    "    pip install pykitti\n",
    "    pip install pyproj\n",
    "    ```\n",
    "\n",
    "Если все прошло успешно, то закрываем данный ноутбук, в консоли переключаемся на окружение `sdc`, и находясь в данном окружение запускаемся заново\n",
    "```\n",
    "(sdc) ...$ ls\n",
    "hd_maps.ipynb ...\n",
    "(sdc) ...$ jupyter notebook\n",
    "```\n",
    "После перезапуска все импорты ниже должны работать.\n",
    "\n",
    "> Замечание. Если будут какие-то проблемы с 3D-визуализацией `plotly`, то стоит попробовать запустиь ноутбук с командой `--no-browser`\n",
    "> ```shell\n",
    "> (sdc) ...$ jupyter notebook --no-browser\n",
    "> ```\n",
    "> и уже затем открыть ссылку вида `http://127.0.0.1:8888/tree?token=...`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Специальные import-ы\n",
    "import plotly\n",
    "import open3d\n",
    "import pykitti\n",
    "import pyproj\n",
    "import cv2\n",
    "\n",
    "from IPython.display import (\n",
    "    display,\n",
    "    SVG,\n",
    "    HTML,\n",
    "    Image,\n",
    "    Video,\n",
    "    YouTubeVideo,\n",
    ")\n",
    "\n",
    "import typing as T\n",
    "import os\n",
    "import copy\n",
    "import tqdm\n",
    "import itertools\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='_toc'></a>\n",
    "# Содержание\n",
    "* [Лидары](#introduction)\n",
    "    * [Примеры лидаров](#lidars_examples)\n",
    "    * [Принцип работы](#lidars_operating_principles)\n",
    "    * [Данные лидара](#lidars_data)\n",
    "    * [Источники ошибок](#lidars_measurement_errors)\n",
    "* [Лидарные облака](#lidars_clouds)\n",
    "    * [Визуализация облаков](#lidars_clouds_visualization)\n",
    "    * [Операции над облаками](#lidars_clouds_operations)\n",
    "        * [Линейные преобразования](#linear_transformations)\n",
    "        * [Фильтрация](#pcl_filtration)\n",
    "        * [Оценка фичей](#pcl_features_estimation)\n",
    "        * [Геометрическая сегментация](#geometrical_segmentation)\n",
    "* [Оценка позы](#pose_estimation)\n",
    "    * [Point set registration](#point_set_registration)\n",
    "    * [Iterative Closest Point (ICP)](#icp)\n",
    "* [KITTI](#kitti)\n",
    "    * [Загрузка сцен](#kitti_loading)\n",
    "    * [Содержимое сцен](#kitti_content)\n",
    "        * [Изображения](#kitti_images)\n",
    "        * [Облака](#kitti_clouds)\n",
    "        * [Локализация](#kitti_localization)\n",
    "        * [Калибровки](#kitti_calibrations)\n",
    "* [Источники](#sources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='lidars'></a>\n",
    "# Лидары<sup>[toc](#_toc)</sup>\n",
    "\n",
    "> **LiDAR &mdash; Light Detection and Ranging**\n",
    "\n",
    "* [Примеры лидаров](#lidars_manufacturers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='lidars_examples'></a>\n",
    "## Примеры лидаров<sup>[toc](#_toc)</sup>\n",
    "* [Velodyne](velodyne)\n",
    "* [Waymo](waymo)\n",
    "* [Yandex](yandex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='velodyne'></a>\n",
    "### Velodyne<sup>[toc](#_toc)</sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('./images/velodyne_lidar_00.png', width='100%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('./images/velodyne_lidar_01.png', width='100%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('./images/velodyne_lidar_cloud_HDL-64E.jpg', width='100%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YouTubeVideo('tZ8WbSNsNaU', width='100%', height=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='waymo'></a>\n",
    "### Waymo<sup>[toc](#_toc)</sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('./images/waymo_lidar_cloud_00.png', width='100%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='yandex_lidars'></a>\n",
    "### Yandex<sup>[toc](#_toc)</sup>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Модели<sup>[toc](#_toc)</sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('./images/yandex_lidar_00.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('./images/yandex_lidar_01.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('./images/yandex_lidar_02.jpeg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='yandex_lidar_clouds'></a>\n",
    "### Облака лидара<sup>[toc](#_toc)</sup>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Так выглядит изображение с лидара. Можно увидеть дороги, пешеходов, припаркованные машины, стоянку самокатов и другие объекты вокруг беспилотного автомобиля**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('./images/yandex_lidar_cloud_00.png', width='100%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('./images/yandex_lidar_cloud_01.jpeg', width='100%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('./images/yandex_lidar_cloud_02.jpeg', width='100%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('./videos/yandex_lidar_02.gif', width='100%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Video('./videos/Yandex Software-Defined LiDAR.mp4', width=970)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YouTubeVideo('lPclzuDtmTs', width='100%', height=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='lidars_operating_principles'></a>\n",
    "## Принцип работы<sup>[toc](#_toc)</sup>\n",
    "* [Time-of-flight ranging](#time_of_flight_ranging)\n",
    "* [Интенсивность](#intensity)\n",
    "* [Сканирование пространства](#scanning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='time_of_flight_ranging'></a>\n",
    "### Time-of-flight ranging<sup>[toc](#_toc)</sup>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лидар работает по принципу [**time-of-flight ranging**-а](https://en.wikipedia.org/wiki/Time-of-flight_camera). По такому же принципу работают радары и сонары.\n",
    "\n",
    "Терминология:\n",
    "* Time-of-Flight\n",
    "* Time-of-Arrival\n",
    "* Round trip time (RTT)\n",
    "* Round trip distance (RTD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(SVG('https://upload.wikimedia.org/wikipedia/commons/f/f1/20200501_Time_of_flight.svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пусть\n",
    "* $t$ &mdash; время от отправки импулься до его получения, т.е. RTT (Round Trip Time)\n",
    "* $c$ &mdash; скорость света\n",
    "Тогда расстояние $r$ до мишени/объекта наблюдения можно __оценить__ следующим образом:\n",
    "$$\n",
    "r \\approx \\frac{c \\Delta t}{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='intensity'></a>\n",
    "### Интенсивность<sup>[toc](#_toc)</sup>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://res.mdpi.com/sensors/sensors-15-28099/article_deploy/html/images/sensors-15-28099-g001-1024.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='scanning'></a>\n",
    "### Сканирование пространства<sup>[toc](#_toc)</sup>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вращение зеркала-отражателя вокруг вертикальной оси позволяет получить 2d-срез окружающего пространства. Чтобы получить паттерн 3d-сканирования добавляют вращение зеркальца вокруг горизонтальной оси.\n",
    "![](videos/general_rotating_lidar.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лучей может быть много:\n",
    "![](https://eckop.com/wp-content/uploads/2018/07/LIDAR_02-1024x366.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Паттерн сканирования велодайна:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/velodyne_lidar_structure_00.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/velodyne_lidar_structure_01.png)\n",
    "\n",
    "[Static Calibration and Analysis of the Velodyne HDL-64E S2 for High Accuracy Mobile Scanning](https://www.mdpi.com/2072-4292/2/6/1610)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='lidars_data'></a>\n",
    "## Данные с лидара<sup>[toc](#_toc)</sup>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Сырые\" показания лидара<sup>[toc](#_toc)</sup>\n",
    "* Горизонатльный поворот зеркальца (galvo horizontal encoder value)\n",
    "* Вертикальный поворот зеркальца (galvo vertical encoder value)\n",
    "* время полета (time-of-flight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сферическая система координат<sup>[toc](#_toc)</sup>\n",
    "\n",
    "Положение точки $P$ в сферической системе координат определяется тройкой $(r, \\theta, \\varphi)$, где\n",
    "* $r \\ge 0$ &mdash; **расстояние** (**range/distance**)\n",
    "* $\\theta \\in [-\\pi/2, \\pi/2]$ &mdash; **зенитный** или **полярный** угол (**elevation angle**)\n",
    "* $\\varphi \\in [0, 2\\pi)$ &mdash; **азимутальный** угол (**azimuth angle**)\n",
    "\n",
    "Иногда встречаются альтернативные обозначения:\n",
    "* $r$ &mdash; **r**ange\n",
    "* $\\alpha$ &mdash; **a**zimuth angle\n",
    "* $\\varepsilon$ &mdash; **e**levation angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(SVG('./images/spherical_coordinate_system.svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Декартова система координат<sup>[toc](#_toc)</sup>\n",
    "\n",
    "\\begin{align*}\n",
    "\\begin{pmatrix}\n",
    "x\\\\\n",
    "y\\\\\n",
    "z\n",
    "\\end{pmatrix} =\n",
    "\\begin{pmatrix}\n",
    "r \\cos\\theta \\cos\\phi \\\\ \n",
    "r \\cos\\theta \\sin\\phi\\\\\n",
    "r \\sin\\theta\n",
    "\\end{pmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "\\begin{pmatrix}\n",
    "r\\\\\n",
    "\\phi\\\\\n",
    "\\theta\\\\\n",
    "\\end{pmatrix} = \\boldh(x, y, z) = \\begin{pmatrix}\n",
    "\\sqrt{x^2 + y^2 + z^2}\\\\\n",
    "\\arctan \\left(\\frac{y}{x}\\right)\\\\\n",
    "\\arcsin \\left(\\frac{z}{\\sqrt{x^2 + y^2 + z^2}}\\right)\n",
    "\\end{pmatrix}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='lidars_measurement_errors'></a>\n",
    "## Источники ошибок<sup>[toc](#_toc)</sup>\n",
    "* Неточность в определении времени прибытия сигнала (из-за peak detector-а)\n",
    "* Неточности в определении внутренних механических частей лидара (зеркальца и т.п.)\n",
    "* Взаимодействие со средой\n",
    "* Взаимодействие с поверхностью отражения\n",
    "\n",
    "Пусть $\\boldp = (x, y, z)^T  \\in \\mathbb{R}^3$ &mdash; реальная точка, от которой отразился сигнал, тогда в первом приближении можно моделировать ошибки наблюдения нормальным шумом:\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "\\hat{r}\\\\\n",
    "\\hat{\\phi}\\\\\n",
    "\\hat{\\theta}\\\\\n",
    "\\end{pmatrix} = \\hat{\\boldh}(x, y, z) = \\boldh(x, y, z) + \\boldq,\n",
    "$$\n",
    "где $\\boldq \\sim \\mathcal{N}(\\boldsymbol{0}, Q)$.\n",
    "\n",
    "Ещё одна причина искажений в наблюдениях &mdash; движение устройства, на котором установлен лидар (робота, машины). В таком случае могут возникать двоения, растяжени и сжатия объектов (впрочем, как и много всего прочего)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='lidars_clouds'></a>\n",
    "# Лидарные облака<sup>[toc](#_toc)</sup>\n",
    "* [Визуализация облаков](#lidar_clouds_visualization)\n",
    "* [Операции над облаками](#lidars_clouds_operations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='lidars_clouds_visualization'></a>\n",
    "## Визуализация облаков<sup>[toc](#_toc)</sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "\n",
    "from pylib.pcl.tools.plotly_visualization import (\n",
    "    create_plotly_figure,\n",
    "    plot_cloud,\n",
    "    apply_min_max_scaling,\n",
    "    convert_values_to_rgba_tuples_f64,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Случайное облако<sup>[toc](#_toc)</sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_points = 100\n",
    "cloud_xyz = np.random.normal(size=(num_points, 3))\n",
    "\n",
    "colors = apply_min_max_scaling(cloud_xyz[:, 2])\n",
    "\n",
    "colors = convert_values_to_rgba_tuples_f64(colors, cmap='viridis')\n",
    "figure = create_plotly_figure(bgcolor='black')\n",
    "plot_cloud(\n",
    "    cloud_xyz,\n",
    "    colors=colors,\n",
    "    labels=np.linalg.norm(cloud_xyz, axis=1),\n",
    "    marker_size=3, figure=figure).show()\n",
    "del num_points, cloud_xyz, colors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bunny<sup>[toc](#_toc)</sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./datasets/pcl/bunny.txt', header=None, sep=' ', names=['x', 'y', 'z'])\n",
    "cloud_xyz = data.to_numpy()\n",
    "x = cloud_xyz[:, 0]\n",
    "y = cloud_xyz[:, 2]\n",
    "z = cloud_xyz[:, 1]\n",
    "cloud_xyz = np.stack([x, y, z]).T\n",
    "print(cloud_xyz.shape)\n",
    "plot_cloud(cloud_xyz, marker_size=5).show()\n",
    "del data, cloud_xyz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='lidars_clouds_operations'></a>\n",
    "## Операции над облаками<sup>[toc](#_toc)</sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.transform import Rotation\n",
    "from pylib.pcl.common.cloud_io import read_point_cloud_o3d, read_point_cloud_xyz\n",
    "from pylib.pcl.common.convert_point_cloud import convert_point_cloud_o3d_to_xyz\n",
    "from pylib.pcl.common.transform_point_cloud import transform_point_cloud_xyz\n",
    "from pylib.pcl.filters.voxel_grid import apply_voxel_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='linear_transformations'></a>\n",
    "### Линейные преобразования<sup>[toc](#_toc)</sup>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Базовые операции**:\n",
    "* Сдвиг (translation)\n",
    "* Поворот (rotation)\n",
    "* Масштабирование (scaling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Композитные операции**:\n",
    "* Афинное преобразование (используется редко, так как не физично)\n",
    "* Изометрическое преобразование (__преобразование системы координат__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='example_stanford_bunny'></a>\n",
    "#### Пример. Stanford Bunny<sup>[toc](#_toc)</sup>\n",
    "\n",
    "http://graphics.stanford.edu/data/3Dscanrep/\n",
    "\n",
    "> **Задача** Загрузить данные сканирования датасета и смержить в единое облако с использованием информации о том, из каких поз производилось сканирование"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Читаем облака:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = './datasets/stanford/bunny/data'\n",
    "clouds_names = [\n",
    "    'bun000.ply',\n",
    "    'bun045.ply',\n",
    "    'bun090.ply',\n",
    "    'bun180.ply',\n",
    "    'bun270.ply',\n",
    "    'top2.ply',\n",
    "    'top3.ply',\n",
    "    'bun315.ply',\n",
    "    'chin.ply',\n",
    "    'ear_back.ply',\n",
    "]\n",
    "\n",
    "cloud_o3d_by_name = {}\n",
    "for cloud_name in clouds_names:\n",
    "    cloud_o3d = read_point_cloud_o3d(os.path.join(dataset_path, cloud_name))\n",
    "    cloud_o3d_by_name[cloud_name] = cloud_o3d\n",
    "    print(cloud_o3d)\n",
    "    del cloud_o3d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Читаем позы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_name = 'bun.conf'\n",
    "\n",
    "pose_by_cloud_name = {}\n",
    "with open(os.path.join(dataset_path, conf_name), 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        line = line.split()\n",
    "        if len(line) == 0:\n",
    "            continue\n",
    "        if line[0] != 'bmesh':\n",
    "            continue\n",
    "        _, cloud_name, x, y, z, qx, qy, qz, qw = line\n",
    "        x = float(x)\n",
    "        y = float(y)\n",
    "        z = float(z)\n",
    "        qx = float(qx)\n",
    "        qy = float(qy)\n",
    "        qz = float(qz)\n",
    "        qw = float(qw)\n",
    "        rotation_matrix = np.linalg.inv(Rotation.from_quat([qx, qy, qz, qw]).as_matrix())\n",
    "        translation_vector = np.array([x, y, z])\n",
    "        transform_matrix = np.eye(4)\n",
    "        transform_matrix[:3, :3] = rotation_matrix\n",
    "        transform_matrix[:3, 3] = translation_vector\n",
    "        pose_by_cloud_name[cloud_name] = transform_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получаем XYZ-облака:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud_xyz_by_name = {}\n",
    "transformed_cloud_xyz_by_name = {}\n",
    "for cloud_name, cloud_o3d in cloud_o3d_by_name.items():\n",
    "    cloud_xyz_by_name[cloud_name] = convert_point_cloud_o3d_to_xyz(cloud_o3d)\n",
    "    transformed_cloud_xyz_by_name[cloud_name] =\\\n",
    "        transform_point_cloud_xyz(cloud_xyz_by_name[cloud_name], pose_by_cloud_name[cloud_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мержим трансформированные облака в единое облако:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_cloud_xyz = []\n",
    "for cloud_name in clouds_names:\n",
    "    merged_cloud_xyz.append(transformed_cloud_xyz_by_name[cloud_name])\n",
    "merged_cloud_xyz = np.vstack(merged_cloud_xyz)\n",
    "print(f'Merged cloud shape: {merged_cloud_xyz.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отрисовывать облако такого размера для `plotly` &mdash; довольно сложная задача. Поэтому уменьшим его размер с помощью вокселизации:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Merged cloud shape before voxel grid: {merged_cloud_xyz.shape}')\n",
    "merged_cloud_xyz = apply_voxel_grid(merged_cloud_xyz, voxel_size=0.0015)\n",
    "print(f'Merged cloud shape after voxel grid: {merged_cloud_xyz.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Визуализируем смерженное вокселизируемое облако:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = create_plotly_figure()\n",
    "plot_cloud(merged_cloud_xyz, figure=figure)\n",
    "figure.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='pcl_filtration'></a>\n",
    "### Фильтрация<sup>[toc](#_toc)<sup>\n",
    "    \n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='pcl_features_estimation'></a>\n",
    "### Оценка признаков<sup>[toc](#_toc)</sup>\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='geometrical_segmentation'></a>\n",
    "### Геометрическая сегментация<sup>[toc](#_toc)</sup>\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='pose_estimation'></a>\n",
    "# Оценка позы<sup>[toc](#_toc)</sup>\n",
    "* [Point set registration](#point_set_registration)\n",
    "* [Iterative Closest Point (ICP)](#icp)\n",
    "* [Проблемы ICP](#icp_problems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Point set registration<sup>[toc](#_toc)</sup>\n",
    "\n",
    "Как заматчить два облака?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='icp'></a>\n",
    "## Iterative Closest Point<sup>[toc](#_toc)</sup>\n",
    "\n",
    "https://www.open3d.org/docs/release/tutorial/pipelines/icp_registration.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Идея__. Если хороших соответствий больше, чем плохих, то в процессе оптмизации их будет становиться все больше вплоть до сходимости процесса"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Алгоритм__\n",
    "1. Хорошее начальное приближение. Возможно из constant velocity model или zero velocity motion model\n",
    "2. Построение соответствий (association, correspondences estimation)\n",
    "3. Оценка трансформа (transform estimation)\n",
    "4. Продолжаем итерации вплоть до сходимости"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Варианты ICP__\n",
    "* Point to point\n",
    "* Point to plane\n",
    "* Plane to plane\n",
    "* Point to distr\n",
    "* Distr to distr (GICP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='icp_problems'></a>\n",
    "## Проблемы ICP<sup>[toc](#_toc)</sup>\n",
    "* Выбросы и шумы в самих облаках\n",
    "* Неточные соответствия\n",
    "* Движения машины\n",
    "* Движение вокруг машины"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='kitti'></a>\n",
    "# KITTI<sup>[toc](#_toc)</sup>\n",
    "\n",
    "Датасеты:\n",
    "* https://www.thinkautonomous.ai/blog/lidar-datasets/\n",
    "* https://www.argoverse.org/av2.html\n",
    "* https://www.kaggle.com/datasets/klemenko/kitti-dataset/data\n",
    "\n",
    "Мы будем работать с KITTI. Основной сайт https://www.cvlibs.net/datasets/kitti/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Конфигурация<sup>[toc](#_toc)</sup>\n",
    "\n",
    "![](https://www.cvlibs.net/datasets/kitti/images/setup_top_view.png)\n",
    "\n",
    "![](https://www.cvlibs.net/datasets/kitti/images/passat_sensors_920.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='kitti_loading'></a>\n",
    "## Загрузка сцен<sup>[toc](#_toc)</sup>\n",
    "Нужно скачать набор данных со страницы http://www.cvlibs.net/datasets/kitti/raw_data.php\n",
    "\n",
    "Содержимое датасетов:\n",
    "* Raw (unsynced+unrectified) and processed (synced+rectified) grayscale stereo sequences (0.5 Megapixels, stored in png format)\n",
    "* Raw (unsynced+unrectified) and processed (synced+rectified) color stereo sequences (0.5 Megapixels, stored in png format)\n",
    "* 3D Velodyne point clouds (100k points per frame, stored as binary float matrix)\n",
    "* 3D GPS/IMU data (location, speed, acceleration, meta information, stored as text file)\n",
    "* Calibration (Camera, Camera-to-GPS/IMU, Camera-to-Velodyne, stored as text file)\n",
    "* 3D object tracklet labels (cars, trucks, trams, pedestrians, cyclists, stored as xml file)\n",
    "\n",
    "#### 2011_09_26_drive_0002 (0.3 GB)\n",
    "* [unsynced+unrectified data](https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0002/2011_09_26_drive_0002_extract.zip)\n",
    "* [synced+rectified data](https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0002/2011_09_26_drive_0002_sync.zip)\n",
    "* [calibration](https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_calib.zip)\n",
    "* [tracklets](https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0002/2011_09_26_drive_0002_tracklets.zip)\n",
    "\n",
    "#### 2011_09_26_drive_0106 (0.9 GB)\n",
    "* [unsynced+unrectified data](https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0106/2011_09_26_drive_0106_extract.zip)\n",
    "* [synced+rectified data](https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0106/2011_09_26_drive_0106_sync.zip)\n",
    "* [calibration](https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_calib.zip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для удобной работы с выкаченным датасетом есть библиотека `pykitti` (https://github.com/utiasSTARS/pykitti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KITTI_DIR_PATH = os.path.abspath('./datasets/KITTI/')\n",
    "print(f'KITTI dataset base dir: <{KITTI_DIR_PATH}>')\n",
    "\n",
    "# Указываем датасет для загрузки\n",
    "RIDE_DATE = '2011_09_26'\n",
    "DRIVE = '0002'\n",
    "\n",
    "# Загружаем данные. Опционально можно указать диапазон фреймов для загрузки\n",
    "KITTI_DATASET = pykitti.raw(base_path=KITTI_DIR_PATH, date=RIDE_DATE, drive=DRIVE)\n",
    "\n",
    "# Сразу же создаем адаптор\n",
    "KITTI_DATASET_ADAPTOR = KittiDatasetAdaptor(KITTI_DATASET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='kitti_content'></a>\n",
    "## Содержимое сцен<sup>[toc](#_toc)</sup>\n",
    "* [Изображения](#kitti_images)\n",
    "* [Облака](#kitti_clouds)\n",
    "* [Локализация](#kitti_localization)\n",
    "* [Калибровки](#kitti_calibrations)\n",
    "\n",
    "Далее посмотрим на то, что содержится в загруженном датасете KITTI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='kitti_images'></a>\n",
    "## Изображения<sup>[toc](#_toc)</sup>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**В датасете содержатся данные для 4-х камер**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of camera 0 images: {len(KITTI_DATASET.cam0_files)}')\n",
    "print(f'Number of camera 1 images: {len(KITTI_DATASET.cam1_files)}')\n",
    "print(f'Number of camera 2 images: {len(KITTI_DATASET.cam2_files)}')\n",
    "print(f'Number of camera 3 images: {len(KITTI_DATASET.cam3_files)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Посмотрим на ректифицированные изображения камер:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_idx = 0\n",
    "image0 = KITTI_DATASET.get_cam0(frame_idx)\n",
    "image1 = KITTI_DATASET.get_cam1(frame_idx)\n",
    "image2 = KITTI_DATASET.get_cam2(frame_idx)\n",
    "image3 = KITTI_DATASET.get_cam3(frame_idx)\n",
    "\n",
    "plt.figure(figsize=(20, 8))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.imshow(image0, cmap='gray')\n",
    "plt.title('camera 0')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.imshow(image1, cmap='gray')\n",
    "plt.title('camera 1')\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.imshow(image2)\n",
    "plt.title('camera 2')\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.imshow(image3)\n",
    "plt.title('camera 3');\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "del frame_idx, image0, image1, image2, image3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_idx = 0\n",
    "image = KITTI_DATASET.get_cam2(1)\n",
    "print(f'Image type={type(image)}, image size={image.size}')\n",
    "plt.figure(figsize=(16, 12))\n",
    "plt.imshow(image)\n",
    "del frame_idx, image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ректифицированные изображения &mdash; это изображения в стерео-плоскости. Все камеры приведены к одной стерео-плоскости. Это нужно учитывать при проекции лидарных точек на ректифицированные изображения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Посмотрим на изображения цветной стерео-пары:**<sup>[toc](#_toc)</sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_idx = 1\n",
    "image1, image2 = KITTI_DATASET.get_rgb(frame_idx)\n",
    "print(f'image1 type={type(image1)}, image1 size={image1.size}')\n",
    "print(f'image2 type={type(image2)}, image2 size={image2.size}')\n",
    "\n",
    "plt.figure(figsize=(16, 12))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.imshow(image1)\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.imshow(image2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='kitti_clouds'></a>\n",
    "### Облака<sup>[toc](#_toc)</sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загружаем облако из датасета\n",
    "cloud_idx = 0\n",
    "cloud_xyzi = KITTI_DATASET.get_velo(cloud_idx)\n",
    "print(type(cloud_xyzi), cloud_xyzi.shape, cloud_xyzi.dtype)\n",
    "\n",
    "# Визуализируем облако\n",
    "figure = create_plotly_figure(bgcolor='black')\n",
    "\n",
    "colors = apply_min_max_scaling(cloud_xyzi[:, 3], min_value=0.2, max_value=1.0)\n",
    "colors = colors**(2/3.)  # Гамма-коррекция цветов \n",
    "colors = convert_values_to_rgba_tuples_f64(colors, cmap='hot')\n",
    "assert colors.shape == (cloud_xyzi.shape[0], 4)\n",
    "\n",
    "labels = [f'i={intensity:3f}' for intensity in cloud_xyzi[:, 3]]\n",
    "\n",
    "plot_cloud(\n",
    "    cloud=cloud_xyzi,\n",
    "    colors=colors,\n",
    "    figure=figure,\n",
    "    labels=labels,\n",
    ").show()\n",
    "\n",
    "del colors, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Визуализация трех последовательных облаков<sup>[toc](#_toc)</sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cloud0 = KITTI_DATASET.get_velo(0)\n",
    "cloud1 = KITTI_DATASET.get_velo(1)\n",
    "cloud2 = KITTI_DATASET.get_velo(2)\n",
    "\n",
    "figure = create_plotly_figure(bgcolor='black')\n",
    "plot_cloud(cloud0, colors='red', figure=figure)\n",
    "plot_cloud(cloud1, colors='green', figure=figure)\n",
    "plot_cloud(cloud2, colors='blue', figure=figure)\n",
    "figure.show()\n",
    "\n",
    "del cloud0, cloud1, cloud2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='kitti_localization'></a>\n",
    "### Локализация<sup>[toc](#_toc)</sup>\n",
    "\n",
    "Результаты локализации лежат в `oxts` файлах. Описание формата можно посмотреть [здесь](https://github.com/pratikac/kitti/blob/master/readme.raw.txt).\n",
    "\n",
    "Для преобразования между различными системами геопозиционирования нам потребуется библиотека `pyproj`, установка которой проведена во время настройки питоновского окружения.\n",
    "\n",
    "Считывание показаний из датасета состоит из несольких шагов:\n",
    "1. Считываем latitude, longitude, altitude положений GPS-сенсора (LLA-геопозиции)\n",
    "2. Создаем [проекцию меркатора](https://en.wikipedia.org/wiki/Mercator_projection), где в качестве референсной точки выбираем первое показание GPS-сенсора\n",
    "3. Конвертируем LLA-геопозиции в XYZ-геопозиции в системе координат меркатора\n",
    "4. Считываем ориентации GPS-сенсора в системе координат меркатора\n",
    "5. Формируем окончательные значения показаний локализации\n",
    "\n",
    "Эти шаги детально рассматривались на лекции/семинаре. Здесь же просто воспользуемся классом `KittiDatasetAdaptor`, чтобы просто получить окончетельный набор поз GPS-сенсора."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylib.geo.geo_lla_xyz_converter import GeoLlaXyzConverter\n",
    "from pylib.geo.geo_position_lla import GeoPositionLLA\n",
    "from pylib.geo.geo_position_xyz import GeoPositionXYZ\n",
    "from pylib.kitti.dataset_adaptor import KittiDatasetAdaptor\n",
    "from pylib.kitti.localization import (\n",
    "    Localization,\n",
    "    build_localization,\n",
    "    build_localizations,\n",
    ")\n",
    "from pylib.kitti.convert_localization import (\n",
    "    convert_localization_to_transform_matrix,\n",
    "    convert_localizations_to_transform_matrices,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KITTI_DATASET_ADAPTOR = KittiDatasetAdaptor(KITTI_DATASET)\n",
    "reference_point = KITTI_DATASET_ADAPTOR.read_geo_positions_lla()[0]\n",
    "localizations = KITTI_DATASET_ADAPTOR.build_localizations(reference_point)\n",
    "del reference_point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Пример работы GeoLlaXyzConverter-а"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Инициализируем конвертер между LLA-геопозициями и XYZ-геопозициями в системе координат меркатора\n",
    "geo_lla_xyz_converter = GeoLlaXyzConverter(geo_positions_lla[0])\n",
    "print(f'geo_lla_xyz_converter.reference_point: {geo_lla_xyz_converter.reference_point}')\n",
    "\n",
    "# Проверяем работу конвертера между проекцией меркатора и latlong-системой координат\n",
    "print(geo_lla_xyz_converter.convert_xyz_to_lla(GeoPositionXYZ(0., 0., 0.)))\n",
    "print(geo_lla_xyz_converter.convert_lla_to_xyz(geo_lla_xyz_converter.reference_point))\n",
    "del geo_lla_xyz_converter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Визуализация траекторию, построенной по XYZ-позициями GPS-сенсора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = [g.x for g in geo_positions_xyz]\n",
    "ys = [g.y for g in geo_positions_xyz]\n",
    "zs = [g.z for g in geo_positions_xyz]\n",
    "\n",
    "fig = create_plotly_figure()\n",
    "fig.add_scatter3d(**dict(\n",
    "    x=xs,\n",
    "    y=ys,\n",
    "    z=zs,\n",
    "    mode='lines+markers',\n",
    "    line=dict(color='red', width=2),\n",
    "    marker=dict(size=5),\n",
    "))\n",
    "fig.show()\n",
    "del xs, ys, zs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='kitti_calibrations'></a>\n",
    "### Калибровки<sup>[toc](#_toc)</sup>\n",
    "* Описание расположения сенсоров http://www.cvlibs.net/datasets/kitti/setup.php\n",
    "* Описание содержимого файлов с калибровками https://github.com/yanii/kitti-pcl/blob/master/KITTI_README.TXT\n",
    "    * `T_cam2_velo` &mdash; матрица перехода из лидара в камеру\n",
    "    * `R_rect_20` &mdash; что это?\n",
    "    * `P_rect_20` &mdash; матрица проекций ректифицированного изображения\n",
    "  \n",
    " * Камера 0 выступает в качестве референсной системы координат. Положение лидара в системе координат камеры 0  содержится в файле `calib_velo_to_cam.txt` (аттрибут `T_cam0_velo_unrect`).\n",
    "\n",
    "\n",
    "В KITTI в качестве ректифицированных изображений рассматриваются \"выровненные\" в плоскости стереопары изображения. Так как плоскость стереопары в общем случае повернута относительно исходной плоскости изображения камеры, то в дело вступает матрица ректификации. Матрица перехода из системы координат лидара в систему координат камеры $i$ имеет следующий вид:\n",
    "$$\n",
    "T_{l \\to c_i} = R_{i} T_{c_0 \\to c_i} T_{l \\to c_0},\n",
    "$$\n",
    "где\n",
    "* $T_{l \\to c_0}$ &mdash; матрица перехода из системы координат лидара в систему координат камеры $0$\n",
    "* $T_{c_0 \\to c_i}$ &mdash; матрица перехода из системы координат лидара в систему координат камер $i$\n",
    "* $R_i$ &mdash; матрица ректификации для камеры $i$ (матрица поворота, которая выравнивает камеры между собой, переводя их к одной плоскости изображения)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obtained = np.dot(DATASET.calib.R_rect_00, DATASET.calib.T_cam0_velo_unrect) \n",
    "expected = DATASET.calib.T_cam0_velo\n",
    "assert np.max(np.abs(expected - obtained)) < 1e-10\n",
    "print(DATASET.calib.T_cam0_velo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Матрицы ректификации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for camera_idx in range(4):\n",
    "    R_rect_name = 'R_rect_{}0'.format(camera_idx)\n",
    "    print('{}:\\n{}\\n'.format(R_rect_name, getattr(DATASET.calib, R_rect_name)))\n",
    "    del camera_idx, R_rect_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Матрицы ректификации у всех камер отличаются**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Матрицы проекций"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_rect = {}\n",
    "for camera_idx in range(4):\n",
    "    P_rect_name = 'P_rect_{}0'.format(camera_idx)\n",
    "    P_rect[camera_idx] = getattr(DATASET.calib, P_rect_name)\n",
    "    print('{}:\\n{}\\n'.format(P_rect_name, P_rect[camera_idx]))\n",
    "    del camera_idx\n",
    "\n",
    "for first, second in itertools.product(range(4), range(4)):\n",
    "    assert np.max(np.abs(P_rect[first] - P_rect[second])[:3, :3]) == 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Основная часть у всех матриц проекций идентична, что, в принципе, логично, так как плоскость Z = 1 у стерео-камер идентична.\n",
    "* Камера 2 единственная расположена левее камеры 0, именно поэтому элемент P[0, 3] у этой камеры положителен, в отличие от камер 1 и 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera_idx = 2\n",
    "np.set_printoptions(precision=5)\n",
    "L2C_name = f'T_cam{camera_idx}_velo'\n",
    "K_name = f'K_cam{camera_idx}'\n",
    "P_rect_name = f'P_rect_{camera_idx}0'\n",
    "R_rect_name = f'R_rect_{camera_idx}0'\n",
    "\n",
    "print('Lidar to camera transform:\\n{}\\n'.format(getattr(DATASET.calib, L2C_name)))\n",
    "print('{}:\\n{}\\n'.format(K_name, getattr(DATASET.calib, K_name)))\n",
    "print('{}:\\n{}\\n'.format(R_rect_name, getattr(DATASET.calib, R_rect_name)))\n",
    "print('{}:\\n{}\\n'.format(P_rect_name, getattr(DATASET.calib, P_rect_name)))\n",
    "del camera_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://medium.com/swlh/camera-lidar-projection-navigating-between-2d-and-3d-911c78167a94\n",
    "* https://github.com/darylclimb/cvml_project/tree/master/projections/lidar_camera_projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pykitti.utils import read_calib_file\n",
    "\n",
    "CALIB = {}\n",
    "CALIB['T_lidar_to_unrect0'] = DATASET.calib.T_cam0_velo_unrect\n",
    "\n",
    "cam_to_cam_calib = read_calib_file(os.path.join(KITTI_DIR_PATH, '2011_09_26', 'calib_cam_to_cam.txt'))\n",
    "for camera_index in range(4):\n",
    "    T = np.eye(4)\n",
    "    T[:3, :3] = cam_to_cam_calib[f'R_0{camera_index}'].reshape(3, 3)\n",
    "    T[:3, 3] = cam_to_cam_calib[f'T_0{camera_index}']\n",
    "    CALIB[f'T_lidar_to_unrect{camera_index}'] = np.dot(T, CALIB['T_lidar_to_unrect0'])\n",
    "    \n",
    "    R_rect = np.eye(4)\n",
    "    R_rect[:3, :3] = cam_to_cam_calib[f'R_rect_0{camera_index}'].reshape(3, 3)\n",
    "    CALIB[f'T_lidar_to_rect{camera_index}'] = np.dot(R_rect, CALIB[f'T_lidar_to_unrect{camera_index}'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CAMERAS = 4\n",
    "for camera_idx in range(NUM_CAMERAS):\n",
    "    print(CALIB['T_lidar_to_rect1'])\n",
    "    print(DATASET.calib.T_cam1_velo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Посмотрим на расположение камер\n",
    "\n",
    "Камера 2 самая левая согласно расположению сенсоров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(DATASET.calib.T_cam2_velo - DATASET.calib.T_cam0_velo)\n",
    "print(DATASET.calib.T_cam2_velo - DATASET.calib.T_cam3_velo)\n",
    "print(DATASET.calib.T_cam2_velo - DATASET.calib.T_cam1_velo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом, все +- согласно ожидаемому расположению:\n",
    "* Обе камеры стоят параллельно по осям\n",
    "* Камера 2 стоит левее камеры 0 на 6.2 см\n",
    "* Камера 2 стоит левее камеры 3 на 53.2 см"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(DATASET.calib.P_rect_20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=5)\n",
    "print(DATASET.calib.T_cam2_velo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylib.project_points import (\n",
    "    project_points_on_image,\n",
    "    project_points_on_cylinder,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='kitti_utils_projection_on_image'></a>\n",
    "### Проектирование лидарных точек на изоображение<sup>[toc](#_toc)</sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_idx = 34\n",
    "camera_idx = 2\n",
    "\n",
    "image = getattr(DATASET, f'get_cam{camera_idx}')(frame_idx)\n",
    "cloud = DATASET.get_velo(frame_idx)\n",
    "n_points = cloud.shape[0]\n",
    "\n",
    "print('Image: type={}, size={}'.format(type(image), image.size))\n",
    "print('Cloud: type={}, shape={}'.format(type(cloud), cloud.shape))\n",
    "\n",
    "projected_points, depths, valid_points_mask = project_points_on_image(\n",
    "    image=image,\n",
    "    cloud=cloud,\n",
    "    lidar_to_camera_transform=DATASET.calib.T_cam0_velo,\n",
    "    rectification_matrix=np.eye(4),\n",
    "    projection_matrix=getattr(DATASET.calib, f'P_rect_{camera_idx}0'),\n",
    ")\n",
    "\n",
    "print('Projected points shape: {}'.format(projected_points.shape))\n",
    "print('Points in image canvas: {}'.format(np.sum(valid_points_mask)))\n",
    "\n",
    "_, axes = plt.subplots(2, 1, figsize=(40, 20))\n",
    "\n",
    "axes[0].imshow(image)\n",
    "axes[1].imshow(image)\n",
    "axes[1].scatter(\n",
    "    projected_points[valid_points_mask, 0],\n",
    "    projected_points[valid_points_mask, 1],\n",
    "    c=depths[valid_points_mask],\n",
    "    s=3)\n",
    "del frame_idx, camera_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='kitti_utils_projection_on_scan'></a>\n",
    "### Projecting lidar points on scan<sup>[toc](#_toc)</sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_idx = 34\n",
    "image = DATASET.get_cam2(frame_idx)\n",
    "points_xyzi = DATASET.get_velo(frame_idx)\n",
    "print('Image is of type: {}'.format(type(image)))\n",
    "print('Cloud is of type: {}'.format(type(points_xyzi)))\n",
    "\n",
    "_, _, valid_points_mask = project_points_on_image(\n",
    "    image=image,\n",
    "    cloud=points_xyzi,\n",
    "    lidar_to_camera_transform=DATASET.calib.T_cam0_velo,\n",
    "    rectification_matrix=np.eye(4),\n",
    "    projection_matrix=DATASET.calib.P_rect_20)\n",
    "print('Points in image canvas: {}'.format(np.sum(valid_points_mask)))\n",
    "print('Projected points shape: {}'.format(projected_points.shape))\n",
    "\n",
    "valid_points_xyzi = points_xyzi[valid_points_mask]\n",
    "valid_points_xyz = valid_points_xyzi[:, :3]\n",
    "valid_depths = np.linalg.norm(valid_points_xyz, axis=1)\n",
    "valid_intensities = valid_points_xyzi[:, 3]\n",
    "\n",
    "num_lines = 64\n",
    "num_columns = 640\n",
    "scan, points_to_cylinder_map = project_points_on_cylinder(\n",
    "    points=valid_points_xyzi,\n",
    "    attributes=[0.05 * valid_depths, valid_intensities],\n",
    "    num_lines=num_lines,\n",
    "    num_columns=num_columns)\n",
    "assert scan.shape == (3, num_lines, num_columns)\n",
    "\n",
    "num_collisions = np.sum(scan[2] > 1)\n",
    "num_points = np.sum(scan[2])\n",
    "assert valid_points_xyz.shape[0] == num_points\n",
    "\n",
    "print('points number:       {}'.format(points_xyzi.shape[0]))\n",
    "print('valid points number: {}'.format(num_points))\n",
    "print('collisions number:   {}'.format(num_collisions))\n",
    "print('collisions rate:     {}'.format(float(num_collisions) / num_points))\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.imshow(np.sqrt(scan[0]), cmap='afmhot')\n",
    "\n",
    "del frame_idx, image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='sources'></a>\n",
    "# Источники<sup>[toc](#_toc)</sup>\n",
    "* [Блог Такси. Яндекс разрабатывает лидары](https://taxi.yandex.ru/blog/lidar)\n",
    "* [Блог Яндекса. Беспилотный флот Яндекса перешёл на собственные лидары: почему это важно и что в них особенного](https://yandex.ru/blog/company/bespilotnyy-flot-yandeksa-pereshel-na-sobstvennye-lidary-pochemu-eto-vazhno-i-chto-v-nikh-osobennogo)\n",
    "\n",
    "* [Хабр. Беспилотный автомобиль: оживляем алгоритмы. Доклад Яндекса](https://habr.com/ru/companies/yandex/articles/471636/)\n",
    "* [Хабр. Как Яндекс делает обычные автомобили беспилотными](https://habr.com/ru/companies/yandex/articles/585444/)\n",
    "* [Хабр. Нейронные сети для планирования движения беспилотных автомобилей](https://habr.com/ru/companies/yandex/articles/763348/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
